{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição\n",
    "\n",
    "Este notebook tem como principal função estruturar uma forma de organizar dados obtidos por meio de busca de artigos (revisão). Para automatizar este processo, o buscador Google Scholar foi escolhido. Como não estava disponível uma API para extração de dados, foi desenvolvido um Web Crawler para extrair os dados de artigos encontrados pelo buscador, organizando-os em uma planilha eletrônica para manipulação e consulta posterior.\n",
    "\n",
    "A lista de queries é a principal variável de controle deste notebook, uma vez que define quais serão os termos pesquisados no buscador. Por definição, dado que o objetivo não é uma revisão sistemática, a requisição utiliza como padrão o algoritmo de ordenação por relevância do Google e acessa até os 70 primeiros resultados por query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerencia os diretórios e caminho de arquivo excel\n",
    "today = date.today().isoformat()\n",
    "today_path = f\"{BASE_PATH}/data/google_search/{today}\"\n",
    "pages_path = f\"{BASE_PATH}/data/google_search/{today}/pages\"\n",
    "excel_path = f\"{BASE_PATH}/data/google_search/{today}/search_results.xlsx\"\n",
    "\n",
    "if not os.path.isdir(today_path):\n",
    "    os.mkdir(today_path)\n",
    "\n",
    "if not os.path.isdir(pages_path):\n",
    "    os.mkdir(pages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'agriculture+and+\"linear+programming\"',\n",
    "    '\"crops+pattern\"+and+\"linear+programming\"',\n",
    "    '\"crop+rotation\"+and+\"linear+programming\"',\n",
    "    '\"land+allocation\"+and+\"linear+programming\"',\n",
    "    '\"land+allocation\"+and+\"linear+programming\"+and+\"crop-livestock\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando Query 5, página 1\r"
     ]
    }
   ],
   "source": [
    "# Controlam a quantidade de resultados em cada página\n",
    "# e até onde será extraído\n",
    "# Fixa o download até a página 6, ordenando por meio\n",
    "# da relevância do algoritmo do Google\n",
    "per_page, finish_at = 10, 60\n",
    "\n",
    "for i in range(0, len(queries)):\n",
    "    query = queries[i]\n",
    "    \n",
    "    for page in range(0, finish_at + per_page, per_page):\n",
    "        # Exibe log de processamento\n",
    "        page_number = 1 + int(page / per_page)\n",
    "        print(f\"Processando Query {i + 1}, página {page_number}\", end=\"\\r\")\n",
    "\n",
    "        # Estrutura a url do google scholar para download de arquivos\n",
    "        # Utiliza como ordenador a função de relevância do algoritmo do Google\n",
    "        # Filtra apenas artigos publicados a partir de 2020\n",
    "        # Não inclui resultados que são apenas citações, todo retorno é de material original\n",
    "        url = f\"https://scholar.google.com/scholar?lr=&q={query}&as_sdt=0,5&as_ylo=2020&as_vis=1&start={page}\"\n",
    "        req = requests.get(url)\n",
    "\n",
    "        # Instancia web crawler e acessa listagem de resultados da busca\n",
    "        soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "        search_results = soup.findAll(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        # Não existem resultados na página, encerra o laço para paginação\n",
    "        if len(search_results) == 0:\n",
    "            break\n",
    "\n",
    "        # Salva o arquivo para leitura posterior\n",
    "        with open(f\"{pages_path}/query_{i + 1}_page_{page_number}.html\", \"w+\") as file:\n",
    "            file.write(str(soup.prettify()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo de listagem de artigos finalizado\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(queries)):\n",
    "    start_at = datetime.now()\n",
    "    query = queries[i]\n",
    "    query_data = []\n",
    "\n",
    "    for page in range(0, finish_at + per_page, per_page):\n",
    "        # Exibe log de processamento\n",
    "        page_number = 1 + int(page / per_page)\n",
    "        print(f\"Processando Query {i + 1}, página {page_number}\", end=\"\\r\")\n",
    "\n",
    "        # Instancia web crawler acessando arquivos locais\n",
    "        file_path = f\"{pages_path}/query_{i + 1}_page_{page_number}.html\"\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "            search_results = soup.findAll(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        # Percorre a lista de resultados, armazenando título do trabalho,\n",
    "        # link de acesso e autores em lista própria\n",
    "        for div in search_results:\n",
    "            title = div.find(\"h3\").find(\"a\")\n",
    "            author = div.find(\"div\", class_=\"gs_a\").text\n",
    "\n",
    "            query_data.append({\n",
    "                \"Título\": title.text.replace(\"\\n\", \"\").replace(\"  \", \"\").strip(),\n",
    "                \"Autor\": author.replace(\"\\n\", \"\").replace(\"  \", \"\").strip(),\n",
    "                \"Link\": title[\"href\"]\n",
    "            })\n",
    "\n",
    "    # Adiciona leituras da query na lista de processados\n",
    "    results.append(query_data)\n",
    "\n",
    "    # Atualiza log de sumarização de busca\n",
    "    end_at = datetime.now()\n",
    "    summary.append({\n",
    "        \"Query\": i + 1,\n",
    "        \"Texto\": query,\n",
    "        \"Início em\": start_at.isoformat(),\n",
    "        \"Término em\": end_at.isoformat(),\n",
    "        \"Duração (s)\": (end_at - start_at).seconds\n",
    "    })\n",
    "\n",
    "with pd.ExcelWriter(excel_path, mode=\"w\") as writer:\n",
    "    # Salva sumário da busca, com textos de queries\n",
    "    # Inicia variável para união de resultados\n",
    "    pd.DataFrame(summary).to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "    union = []\n",
    "\n",
    "    # Define uma forma de busca na lista de união\n",
    "    def search(title: str) -> dict | None:\n",
    "        element = None\n",
    "        for i in range(0, len(union)):\n",
    "            if union[i][\"Título\"] == title:\n",
    "                element = union[i]\n",
    "                break\n",
    "        return element\n",
    "\n",
    "    for i in range(0, len(results)):\n",
    "        # Salva query isolada no arquivo excel\n",
    "        query_data = results[i]\n",
    "        pd.DataFrame(query_data).to_excel(writer, sheet_name=f\"query_{i + 1}\", index=False)\n",
    "\n",
    "        # Faz o processamento na união\n",
    "        for record in query_data:\n",
    "            element = search(record[\"Título\"])\n",
    "            if element is None:\n",
    "                record[\"Query\"] = i + 1\n",
    "                union.append(record)\n",
    "            else:\n",
    "                element[\"Query\"] = f'{element[\"Query\"]},{i + 1}'\n",
    "\n",
    "    # Salva união no arquivo excel\n",
    "    pd.DataFrame(union).to_excel(writer, sheet_name=\"union\", index=False)\n",
    "print(\"Processo de listagem de artigos finalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo de listagem de artigos finalizado\n"
     ]
    }
   ],
   "source": [
    "# Controlam a quantidade de resultados em cada página\n",
    "# e até onde será extraído\n",
    "# Fixa o download até a página 6, ordenando por meio\n",
    "# da relevância do algoritmo do Google\n",
    "per_page, finish_at = 10, 60\n",
    "summary = []\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(queries)):\n",
    "    start_at = datetime.now()\n",
    "    query = queries[i]\n",
    "    query_data = []\n",
    "\n",
    "    for page in range(0, finish_at + per_page, per_page):\n",
    "        # Exibe log de processamento\n",
    "        page_number = 1 + int(page / per_page)\n",
    "        print(f\"Processando Query {i + 1}, página {page_number}\", end=\"\\r\")\n",
    "\n",
    "        # Estrutura a url do google scholar para download de arquivos\n",
    "        # Utiliza como ordenador a função de relevância do algoritmo do Google\n",
    "        # Filtra apenas artigos publicados a partir de 2020\n",
    "        # Não inclui resultados que são apenas citações, todo retorno é de material original\n",
    "        url = f\"https://scholar.google.com/scholar?lr=&q={query}&as_sdt=0,5&as_ylo=2020&as_vis=1&start={page}\"\n",
    "        req = requests.get(url)\n",
    "\n",
    "        # Instancia web crawler e acessa listagem de resultados da busca\n",
    "        soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "        search_results = soup.findAll(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        # Não existem resultados na página, encerra o laço para paginação\n",
    "        if len(search_results) == 0:\n",
    "            break\n",
    "\n",
    "        # Salva o arquivo para leitura posterior\n",
    "        with open(f\"{pages_path}/query_{i + 1}_page_{page_number}.html\", \"w+\") as file:\n",
    "            file.write(str(soup.prettify()))\n",
    "        \n",
    "        # Percorre a lista de resultados, armazenando título do trabalho,\n",
    "        # link de acesso e autores em lista própria\n",
    "        for div in search_results:\n",
    "            title = div.find(\"h3\").find(\"a\")\n",
    "            author = div.find(\"div\", class_=\"gs_a\").text\n",
    "\n",
    "            query_data.append({\n",
    "                \"Título\": title.text,\n",
    "                \"Autor\": author,\n",
    "                \"Link\": title[\"href\"]\n",
    "            })\n",
    "\n",
    "    # Salva artigos extraídos em arquivo excel\n",
    "    results.append(query_data)\n",
    "    '''with pd.ExcelWriter(excel_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        pd.DataFrame(results).to_excel(writer, sheet_name=f\"query_{i + 1}\", index=False)'''\n",
    "\n",
    "    # Atualiza log de sumarização de busca\n",
    "    end_at = datetime.now()\n",
    "    summary.append({\n",
    "        \"Query\": i + 1,\n",
    "        \"Texto\": query,\n",
    "        \"Início em\": start_at.isoformat(),\n",
    "        \"Término em\": end_at.isoformat(),\n",
    "        \"Duração (s)\": (end_at - start_at).seconds\n",
    "    })\n",
    "\n",
    "with pd.ExcelWriter(excel_path, mode=\"w\") as writer:\n",
    "    # Salva sumário da busca, com textos de queries\n",
    "    # Inicia variável para união de resultados\n",
    "    pd.DataFrame(summary).to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "    union = []\n",
    "\n",
    "    # Define uma forma de busca na lista de união\n",
    "    def search(title: str) -> dict | None:\n",
    "        element = None\n",
    "        for i in range(0, len(union)):\n",
    "            if union[i][\"Título\"] == title:\n",
    "                element = union[i]\n",
    "                break\n",
    "        return element\n",
    "\n",
    "    for i in range(0, len(results)):\n",
    "        # Salva query isolada no arquivo excel\n",
    "        query_data = results[i]\n",
    "        pd.DataFrame(query_data).to_excel(writer, sheet_name=f\"query_{i + 1}\", index=False)\n",
    "\n",
    "        # Faz o processamento na união\n",
    "        for record in query_data:\n",
    "            element = search(record[\"Título\"])\n",
    "            if element is None:\n",
    "                record[\"Query\"] = i + 1\n",
    "                union.append(record)\n",
    "            else:\n",
    "                element[\"Query\"] = f'{element[\"Query\"]},{i + 1}'\n",
    "\n",
    "    # Salva união no arquivo excel\n",
    "    pd.DataFrame(union).to_excel(writer, sheet_name=\"union\", index=False)\n",
    "print(\"Processo de listagem de artigos finalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
