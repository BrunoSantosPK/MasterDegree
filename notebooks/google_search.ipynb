{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição\n",
    "\n",
    "Este notebook tem como principal função estruturar uma forma de organizar dados obtidos por meio de busca de artigos (revisão). Para automatizar este processo, o buscador Google Scholar foi escolhido. Como não estava disponível uma API para extração de dados, foi desenvolvido um Web Crawler para extrair os dados de artigos encontrados pelo buscador, organizando-os em uma planilha eletrônica para manipulação e consulta posterior.\n",
    "\n",
    "A lista de queries é a principal variável de controle deste notebook, uma vez que define quais serão os termos pesquisados no buscador. Por definição, dado que o objetivo não é uma revisão sistemática, a requisição utiliza como padrão o algoritmo de ordenação por relevância do Google e acessa até os 70 primeiros resultados por query.\n",
    "\n",
    "OBS: O Google pode bloquear requisições feitas por scripts, para isso uma VPN soluciona. Foi utilizada a Proton VPN quando necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35529/3914482469.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime\n",
    "\n",
    "BASE_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caminhos e diretórios\n",
    "\n",
    "Este bloco tem como função preparar os diretórios e salvar caminhos de arquivos em variáveis utilizadas posteriormente. Para se manter um certo rastreio, cada dia em que este notebook for executado representará uma pasta em dados.\n",
    "\n",
    "As páginas HTML lidas são salvas em um diretório para que sejam processadas posteriormente e componham um histórico de execução dos processos. Um arquivo excel é gerado ao final com o resumo do processamento, bem como informações da execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerencia os diretórios e caminho de arquivo excel\n",
    "today = date.today().isoformat()\n",
    "today_path = f\"{BASE_PATH}/data/google_search/{today}\"\n",
    "pages_path = f\"{BASE_PATH}/data/google_search/{today}/pages\"\n",
    "excel_path = f\"{BASE_PATH}/data/google_search/{today}/search_results.xlsx\"\n",
    "\n",
    "if not os.path.isdir(today_path):\n",
    "    os.mkdir(today_path)\n",
    "\n",
    "if not os.path.isdir(pages_path):\n",
    "    os.mkdir(pages_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de execução\n",
    "\n",
    "Este bloco é onde existe a customização da execução. O principal parâmetro é a lista com queries que serão buscadas. O valor de resultados por página deve ser mantido em 10 (padrão utilizado pelo Google Scholar na sua página gratuita). A quantidade máxima de resultados deve ser um múltiplo de 10 e pode ser alterada de acordo com a necessidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetro para definir as queries que serão utilizadas na pesquisa\n",
    "queries = [\n",
    "    'agriculture+and+\"linear+programming\"',\n",
    "    '\"crops+pattern\"+and+\"linear+programming\"',\n",
    "    '\"crop+rotation\"+and+\"linear+programming\"',\n",
    "    '\"land+allocation\"+and+\"linear+programming\"',\n",
    "    '\"land+allocation\"+and+\"linear+programming\"+and+\"crop-livestock\"'\n",
    "]\n",
    "\n",
    "# Parâmetro para controle do limite de textos que serão extraídos.\n",
    "# O Google Scholar exibe 10 resultados por página, portanto o limite\n",
    "# de finalização representa, também, até que página será acessada.\n",
    "per_page = 10\n",
    "finish_at = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download de páginas\n",
    "\n",
    "Para se ter um melhor controle do processamento, a primeira etapa consiste em fazer o download das páginas com resultados e armazená-las internamente. A variável de diretório definida na seção \"Caminhos e diretórios\" é utilizada aqui. Cada arquivo é salvo com o padrão \"query_{numero_query}\\_page\\_{numero_pagina}.html\". Utiliza um dicionário de log para armazenar dados de execução, utilizado na próxima etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Query 5, página 7\r"
     ]
    }
   ],
   "source": [
    "save_files = dict()\n",
    "\n",
    "for i in range(0, len(queries)):\n",
    "    query = queries[i]\n",
    "    start_at = datetime.now()\n",
    "    save_files[i] = {\"duration\": None, \"start\": None, \"end\": None, \"files\": []}\n",
    "    \n",
    "    for page in range(0, finish_at + per_page, per_page):\n",
    "        # Exibe log de processamento\n",
    "        page_number = 1 + int(page / per_page)\n",
    "        print(f\"Download Query {i + 1}, página {page_number}\", end=\"\\r\")\n",
    "\n",
    "        # Estrutura a url do google scholar para download de arquivos\n",
    "        # Utiliza como ordenador a função de relevância do algoritmo do Google\n",
    "        # Filtra apenas artigos publicados a partir de 2020\n",
    "        # Não inclui resultados que são apenas citações, todo retorno é de material original\n",
    "        url = f\"https://scholar.google.com/scholar?lr=&q={query}&as_sdt=0,5&as_ylo=2020&as_vis=1&start={page}\"\n",
    "        req = requests.get(url)\n",
    "\n",
    "        # Instancia web crawler e acessa listagem de resultados da busca\n",
    "        soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "        search_results = soup.findAll(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        # Não existem resultados na página, encerra o laço para paginação\n",
    "        if len(search_results) == 0:\n",
    "            break\n",
    "\n",
    "        # Salva o arquivo para leitura posterior\n",
    "        file_name = f\"{pages_path}/query_{i + 1}_page_{page_number}.html\"\n",
    "        with open(file_name, \"w+\") as file:\n",
    "            file.write(str(soup.prettify()))\n",
    "\n",
    "        # Registra arquivo no log\n",
    "        save_files[i][\"files\"].append(file_name)\n",
    "\n",
    "    # Registra duração do processo\n",
    "    end_at = datetime.now()\n",
    "    save_files[i][\"end\"] = end_at.isoformat()\n",
    "    save_files[i][\"start\"] = start_at.isoformat()\n",
    "    save_files[i][\"duration\"] = (end_at - start_at).seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento\n",
    "\n",
    "A partir do log de execução anterior, acessa páginas e instancia o web crawler que estrutura resultados em tabelas para serem salvos em excel de controle. Utiliza uma lista para armazenar dados tabulados dos resultados de cada query, assim como para armazenar dados sumarizados da execução da busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis de armazenamento\n",
    "summary: List[dict] = []\n",
    "results: List[List[dict]] = []\n",
    "\n",
    "for i in range(0, len(queries)):\n",
    "    query_data: List[dict] = []\n",
    "    \n",
    "    for file_name in save_files[i][\"files\"]:\n",
    "        # Acessa arquivo salvo e instancia o web crawler\n",
    "        with open(file_name, \"r\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "            search_results = soup.findAll(\"div\", class_=\"gs_r gs_or gs_scl\")\n",
    "\n",
    "        # Percorre a lista de resultados, armazenando título do trabalho,\n",
    "        # link de acesso e autores em lista própria\n",
    "        for div in search_results:\n",
    "            title = div.find(\"h3\").find(\"a\")\n",
    "            author = div.find(\"div\", class_=\"gs_a\").text\n",
    "\n",
    "            query_data.append({\n",
    "                \"Título\": title.text.replace(\"\\n\", \"\").replace(\"  \", \"\").strip(),\n",
    "                \"Autor\": author.replace(\"\\n\", \"\").replace(\"  \", \"\").strip(),\n",
    "                \"Link\": title[\"href\"]\n",
    "            })\n",
    "\n",
    "    # Adiciona leituras da query na lista de processados\n",
    "    results.append(query_data)\n",
    "\n",
    "    # Atualiza log de sumarização de busca\n",
    "    summary.append({\n",
    "        \"Query\": i + 1,\n",
    "        \"Texto\": queries[i],\n",
    "        \"Quantidade de artigos\": len(query_data),\n",
    "        \"Início em\": save_files[i][\"start\"],\n",
    "        \"Término em\": save_files[i][\"end\"],\n",
    "        \"Duração (s)\": save_files[i][\"duration\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo resumo\n",
    "\n",
    "A última etapa deste notebook é compilar todos os dados em um arquivo excel de resumo, contendo as queries utilizadas, tempo para download dos resultados, quantidade de textos encontrados e a união de todos os textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(excel_path, mode=\"w\") as writer:\n",
    "    # Salva sumário da busca, com textos de queries\n",
    "    # Inicia variável para união de resultados\n",
    "    pd.DataFrame(summary).to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "    union: List[dict] = []\n",
    "\n",
    "    # Define uma forma de busca na lista de união\n",
    "    def search(title: str) -> dict | None:\n",
    "        element = None\n",
    "        for i in range(0, len(union)):\n",
    "            if union[i][\"Título\"] == title:\n",
    "                element = union[i]\n",
    "                break\n",
    "        return element\n",
    "\n",
    "    for i in range(0, len(results)):\n",
    "        # Salva query isolada no arquivo excel\n",
    "        query_data = results[i]\n",
    "        pd.DataFrame(query_data).to_excel(writer, sheet_name=f\"query_{i + 1}\", index=False)\n",
    "\n",
    "        # Faz o processamento na união\n",
    "        for record in query_data:\n",
    "            element = search(record[\"Título\"])\n",
    "            if element is None:\n",
    "                record[\"Query\"] = i + 1\n",
    "                union.append(record)\n",
    "            else:\n",
    "                element[\"Query\"] = f'{element[\"Query\"]},{i + 1}'\n",
    "\n",
    "    # Salva união no arquivo excel\n",
    "    pd.DataFrame(union).to_excel(writer, sheet_name=\"union\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprime páginas\n",
    "\n",
    "Para evitar a quantidade de arquivos HMTL neste diretório, o próximo bloco adiciona todos os arquivos baixados em um .zip e remove os arquivos originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca páginas HTML em arquivo zip\n",
    "files = os.listdir(pages_path)\n",
    "with ZipFile(f\"{pages_path}/pages.zip\", \"w\") as zip_file:\n",
    "    for file in files:\n",
    "        zip_file.write(f\"{pages_path}/{file}\")\n",
    "\n",
    "# Remove páginas HTML\n",
    "for file in files:\n",
    "    os.remove(f\"{pages_path}/{file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
